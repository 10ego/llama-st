# llama.cpp Demo Interface with Streamlit

Install the package in `requirements.txt`
streamlit run st-llama.py

You can serve it to a non-default port using `--server.port` flag
