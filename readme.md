# llama.cpp Demo Interface with Streamlit

Install the package in `requirements.txt`
streamlit run st-llama.py

You can serve it to a non-default port using `--server.port` flag


## Note to self
callbacks dir is required right now until this issue can be resolved https://github.com/hwchase17/langchain/pull/6508
